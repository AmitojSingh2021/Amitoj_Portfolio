{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case_Study_Amitoj.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu7BFVUkRsmi",
        "outputId": "2115fbc8-5826-441c-a09a-11d17803ba12"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "!pip install nltk\n",
        "!pip install twython\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting twython\n",
            "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.1)\n",
            "Installing collected packages: twython\n",
            "Successfully installed twython-3.9.1\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLutnOHdpkHj"
      },
      "source": [
        "Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OP9frcQRuwM"
      },
      "source": [
        "df = pd.read_csv(\"customer_reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "CLvi2qS28M0c",
        "outputId": "17c1cc60-b8e3-4173-c3e4-6ab5c007e3a7"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slno</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>a fun adventure. Perfect facilities available...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>As an extended family  we went for a two nigh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Booked this hotel for the 3rd time in 4 years ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Came here for short break. The stay was quite...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Check in process was quick and smooth  with fr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   slno                                               text\n",
              "0     1   a fun adventure. Perfect facilities available...\n",
              "1     2   As an extended family  we went for a two nigh...\n",
              "2     3  Booked this hotel for the 3rd time in 4 years ...\n",
              "3     4   Came here for short break. The stay was quite...\n",
              "4     5  Check in process was quick and smooth  with fr..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mC3OrfOSEZ1"
      },
      "source": [
        "df_tx = pd.read_csv(\"taxonomy.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "XhsQVsba149G",
        "outputId": "d414bac8-64b3-48f6-be48-76234b43ee63"
      },
      "source": [
        "df_tx.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Subtopic</th>\n",
              "      <th>PrimaryKeywords</th>\n",
              "      <th>AdditionalKeywords</th>\n",
              "      <th>ExcludeKeywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Room</td>\n",
              "      <td>Room Cleanliness</td>\n",
              "      <td>hotel*, room*</td>\n",
              "      <td>clean*, neat*, dirt*, maintain*, filth*, decen*</td>\n",
              "      <td>staff*, worker*, lady</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Room</td>\n",
              "      <td>Room Ambience</td>\n",
              "      <td>hotel*, room*</td>\n",
              "      <td>ambiance, ambience, amenities, view</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Staff</td>\n",
              "      <td>Staff Courtesy</td>\n",
              "      <td>staff*, worker*, lady, personne*</td>\n",
              "      <td>friend*, help*, courteo*, polite*, impolite*, ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Room</td>\n",
              "      <td>Wifi</td>\n",
              "      <td>Wifi</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Room</td>\n",
              "      <td>Air Conditioning</td>\n",
              "      <td>ac, \"air condition\", aircon</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Topic  ...        ExcludeKeywords\n",
              "0   Room  ...  staff*, worker*, lady\n",
              "1   Room  ...                    NaN\n",
              "2  Staff  ...                    NaN\n",
              "3   Room  ...                    NaN\n",
              "4   Room  ...                    NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If9Ud78ppoa3"
      },
      "source": [
        "Cleaned the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f1zMnqcSJ-r"
      },
      "source": [
        "def asterix_handler(asterixw, lookupw):\n",
        "    mtch = \"F\"\n",
        "    for word in asterixw:\n",
        "        for lword in lookupw:\n",
        "            if (word[-1:]==\"*\"):\n",
        "                if (bool(re.search(\"^\"+ word[:-1],lword))==True):\n",
        "                    mtch = \"T\"\n",
        "                    break\n",
        "    return (mtch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVincokdSO65"
      },
      "source": [
        "def remov_punct(withpunct):\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    without_punct = \"\"\n",
        "    char = 'nan'\n",
        "    for char in withpunct:\n",
        "        if char not in punctuations:\n",
        "            without_punct = without_punct + char\n",
        "    return (without_punct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3PRqIwpSR-X"
      },
      "source": [
        "def remov_quote(withquote):\n",
        "    quote = '\"'\n",
        "    without_quote = \"\"\n",
        "    char = 'nan'\n",
        "    for char in withquote:\n",
        "        if char not in quote:\n",
        "            without_quote = without_quote + char\n",
        "    return (without_quote)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWXvIzY1SVWV"
      },
      "source": [
        "sentence_data = pd.DataFrame(columns=['slno','text'])\n",
        "for d in range(len(df)):    \n",
        "    doc = (df.iloc[d,1].split('.'))\n",
        "    for s in ((doc)):        \n",
        "        temp = pd.DataFrame({'slno': [df['slno'][d]], 'text': [s]})\n",
        "        sentence_data =  pd.concat([sentence_data, temp])\n",
        "        temp = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubdFP5SnSdjy"
      },
      "source": [
        "sentence_data['text'].replace('',np.nan,inplace=True);      \n",
        "sentence_data.dropna(subset=['text'], inplace=True);  \n",
        "\n",
        "\n",
        "data = sentence_data\n",
        "cat2list = list(set(df_tx['Subtopic']))\n",
        "data['Category'] = 0\n",
        "mapped_data = pd.DataFrame(columns = ['slno','text','Category']);\n",
        "temp=pd.DataFrame()\n",
        "for k in range(len(data)):\n",
        "    comment = remov_punct(data.iloc[k,1])\n",
        "    data_words = [str(x.strip()).lower() for x in str(comment).split()]\n",
        "    data_words = list(filter(None, data_words))\n",
        "    output = []\n",
        "    \n",
        "    for l in range(len(df_tx)):\n",
        "        key_flag = False\n",
        "        and_flag = False\n",
        "        not_flag = False\n",
        "        if (str(df_tx['PrimaryKeywords'][l])!='nan'):\n",
        "            kw_clean = (remov_quote(df_tx['PrimaryKeywords'][l]))\n",
        "        if (str(df_tx['AdditionalKeywords'][l])!='nan'):\n",
        "            aw_clean = (remov_quote(df_tx['AdditionalKeywords'][l]))\n",
        "        else:\n",
        "            aw_clean = df_tx['AdditionalKeywords'][l]\n",
        "        if (str(df_tx['ExcludeKeywords'][l])!='nan'):\n",
        "            nw_clean = remov_quote(df_tx['ExcludeKeywords'][l])\n",
        "        else:\n",
        "            nw_clean = df_tx['ExcludeKeywords'][l]\n",
        "        Key_words = 'nan'\n",
        "        and_words = 'nan'\n",
        "        and_words2 = 'nan'\n",
        "        not_words = 'nan'\n",
        "        not_words2 = 'nan'\n",
        "        \n",
        "        if (str(kw_clean)!='nan'):\n",
        "            key_words = [str(x.strip()).lower() for x in kw_clean.split(',')]\n",
        "            key_words2 = set(w.lower() for w in key_words)\n",
        "        \n",
        "        if (str(aw_clean)!='nan'):\n",
        "            and_words = [str(x.strip()).lower() for x in aw_clean.split(',')]\n",
        "            and_words2 = set(w.lower() for w in and_words)\n",
        "        \n",
        "        if (str(nw_clean)!= 'nan'):\n",
        "            not_words = [str(x.strip()).lower() for x in nw_clean.split(',')]\n",
        "            not_words2 = set(w.lower() for w in not_words)\n",
        "        \n",
        "        if (str(kw_clean) == 'nan'):\n",
        "            key_flag = False        \n",
        "        else:\n",
        "            if set(data_words) & key_words2:\n",
        "                key_flag = True\n",
        "            else:\n",
        "                if (asterix_handler(key_words2, data_words)=='T'):\n",
        "                    key_flag = True\n",
        "                    \n",
        "        if (str(aw_clean)=='nan'):\n",
        "            and_flag = True\n",
        "        else:\n",
        "            if set(data_words) & and_words2:\n",
        "                and_flag = True\n",
        "            else:\n",
        "                if (asterix_handler(and_words2,data_words)=='T'):\n",
        "                    and_flag = True\n",
        "        if (str(nw_clean) == 'nan'):\n",
        "            not_flag = False\n",
        "        else:\n",
        "            if set(data_words) & not_words2:\n",
        "                not_flag = True\n",
        "            else:\n",
        "                if (asterix_handler(not_words2, data_words)=='T'):\n",
        "                    not_flag = True\n",
        "        if (key_flag == True and and_flag == True and not_flag == False):\n",
        "            output.append(str(df_tx['Subtopic'][l]))            \n",
        "            temp = pd.DataFrame({'slno': [data.iloc[k,0]], 'text': [data.iloc[k,1]], 'Category': [df_tx['Subtopic'][l]]})\n",
        "            mapped_data = pd.concat([mapped_data,temp])\n",
        "#output mapped data\n",
        "mapped_data.to_csv(\"mapped_data.csv\",index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULamgCBipu3b"
      },
      "source": [
        "Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0AxFOy3eXg8"
      },
      "source": [
        "def findpolar(test_data):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    polarity = sia.polarity_scores(test_data)[\"compound\"];\n",
        "    if (polarity >= 0.1):    \n",
        "     foundpolar = \"1\"\n",
        "    if (polarity <= -0.1):\n",
        "     foundpolar = \"-1\" \n",
        "    if (polarity>= -0.1 and polarity<= 0.1):\n",
        "     foundpolar = \"0\"\n",
        "    return (foundpolar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDtDQfFCSiEv"
      },
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "mapped_data[\"sentiments\"] = mapped_data[\"text\"].apply(lambda x: sia.polarity_scores(x))\n",
        "sentiment_mapped_data = pd.concat([mapped_data.drop(['sentiments'], axis=1), mapped_data['sentiments'].apply(pd.Series)], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiOCFKflSpXm"
      },
      "source": [
        "sentiment_mapped_data.to_csv(\"sentiment_mapped_data.csv\",index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}